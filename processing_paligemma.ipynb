{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29ccc6a9",
   "metadata": {},
   "source": [
    "## here we forst want to combine the tensors of image and text \n",
    "## let see how we can do that.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af88808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543f0f47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abbd5e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf189449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c83cdc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3444bec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch \n",
    "from typing import List, Dict, Optional, Tuple, Union, Iterable\n",
    "\n",
    "\n",
    "IMAGENET_STANDARD_MEAN = [0.5, 0.5, 0.5]\n",
    "IMAGENET_STANDARD_STD = [0.5, 0.5, 0.5]\n",
    "\n",
    "\n",
    "def add_image_tokens_prompt(prefix_prompt, bos_token, image_seq_len, image_token):\n",
    "    return f\"{image_token * image_seq_len}{bos_token}{prefix_prompt}\\n\"\n",
    "\n",
    "\n",
    "def resize(image, size, resample = None, reducing_gap= None ):\n",
    "    height, width = size[0], size[1]\n",
    "\n",
    "    resized_image = image.resize((width, height), resample=resample)\n",
    "    \n",
    "    return resized_image\n",
    "\n",
    "\n",
    "def rescale(image, scale, dtype=np.float32):\n",
    "\n",
    "    rescaled_image = image*scale\n",
    "\n",
    "    rescaled_image = rescaled_image.astype(dtype)\n",
    "\n",
    "    return rescaled_image\n",
    "\n",
    "def normalize(image, mean, std):\n",
    "    mean = np.array(mean, dtype = image.dtype)\n",
    "    std = np.array(std, dtype = image.dtype)\n",
    "\n",
    "    image = (image - mean)/std\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "\n",
    "def process_images(images, size, resample, rescale_factor, image_mean, image_std):\n",
    "    height, width = size[0], size[1]\n",
    "\n",
    "    images = [resize(image=image, size=(height, width),resample=resample) for image in images]\n",
    "\n",
    "    images = [np.array(image) for image in images]\n",
    "\n",
    "    images = [rescale(image, scale= rescale_factor) for image in images]\n",
    "\n",
    "    images = [normalize(image, mean = image_mean, std = image_std) for image in images]\n",
    "\n",
    "    images = [image.transpose(2,0,1) for image in images]\n",
    "\n",
    "    return images\n",
    "\n",
    "\n",
    "class PaliGemmaProcessor:\n",
    "    IMAGE_TOKEN = \"<image>\"\n",
    "\n",
    "    def __init__(self, tokenizer, num_image_tokens, image_size):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_seq_length = num_image_tokens\n",
    "        self.image_size = image_size\n",
    "\n",
    "        ## additional token added to the tokenizer\n",
    "        token_to_add = {\"additional_special_tokens\": [self.IMAGE_TOKEN]}\n",
    "\n",
    "        tokenizer.add_special_tokens(token_to_add)\n",
    "\n",
    "        ## Extra token for the image\n",
    "\n",
    "        EXTRA_TOKENS = [\n",
    "            f\"<loc{i:04d}>\" for i in range(1024)\n",
    "        ] ## these tokens are used for object detection (bounding boxes)\n",
    "\n",
    "        EXTRA_TOKENS += [\n",
    "            f\"<seg{i:03d}>\" for i in range(128)\n",
    "        ] ## these tokens are used for segmentation\n",
    "\n",
    "        tokenizer.add_tokens(EXTRA_TOKENS)\n",
    "        self.image_token_id = tokenizer.convert_tokens_to_ids(self.IMAGE_TOKEN)\n",
    "\n",
    "        ## we will add the bos and eos token ourselves\n",
    "\n",
    "        tokenizer.add_bos_token = False\n",
    "        tokenizer.add_eos_token = False\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, text, images, padding =\"longest\", truncation = True):\n",
    "        assert len(images) == 1 and len(text) == 1,f\"Received {len(images)} images for {len(text)} prompts.\"\n",
    "\n",
    "        pixel_values = process_images(\n",
    "            images,\n",
    "            size = (self.image_size, self.image_size),\n",
    "            resample = Image.Resampling.BICUBIC,\n",
    "            rescale_factor = 1 / 255.0,\n",
    "            image_mean = IMAGENET_STANDARD_MEAN,\n",
    "            image_std = IMAGENET_STANDARD_STD \n",
    "        )\n",
    "\n",
    "        pixel_values = np.stack(pixel_values, axis=0)\n",
    "        pixel_values = torch.tensor(pixel_values)\n",
    "\n",
    "        ## Prepend a self.image_seq_length number of image tokens to the prompt\n",
    "\n",
    "        input_string = [\n",
    "            add_image_tokens_prompt(\n",
    "                prefix_prompt = prompt,\n",
    "                bos_token = self.tokenizer.bos_token,\n",
    "                image_seq_len = self.image_seq_length,\n",
    "                image_token = self.IMAGE_TOKEN\n",
    "            )\n",
    "            for prompt in text\n",
    "        ]\n",
    "\n",
    "\n",
    "        ## return the input_ids and attention_mask as pytorch tensors\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            input_string,\n",
    "            return_tensors = \"pt\",\n",
    "            padding = padding,\n",
    "            truncation = truncation\n",
    "        )\n",
    "\n",
    "        return_data = {\"pixel_values\":pixel_values, **inputs}\n",
    "\n",
    "        return return_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "solar_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
